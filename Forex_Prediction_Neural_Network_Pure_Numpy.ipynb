{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Forex Prediction Neural Network Pure Numpy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ballmdr/Machine-Learning/blob/master/Forex_Prediction_Neural_Network_Pure_Numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NstsiVAx2t2G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "1bdbdb55-535f-43e9-e0a0-6874b44292c4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDgxb13v2uTU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "ffcc925a-be71-48a9-ca35-76526fc75eb0"
      },
      "source": [
        "filename = '/content/drive/My Drive/export/M30_all_EURUSD.csv'\n",
        "df = pd.read_csv(filename, names=['Date', 'Time', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
        "df['Datetime'] = pd.to_datetime(df.Date + ' ' + df.Time)\n",
        "df.set_index('Datetime', inplace=True)\n",
        "\n",
        "# Features for Momentum Strategy\n",
        "df['Returns'] = - df.Close.pct_change(-1)\n",
        "\n",
        "df['MA35'] = df.Close.rolling(35).mean()\n",
        "df['MA70'] = df.Close.rolling(70).mean()\n",
        "df['MA200'] = df.Close.rolling(200).mean()\n",
        "\n",
        "# ma35 > ma70 = bullish\n",
        "df['35_70'] = np.where(df.MA35 >= df.MA70, 1, 0)\n",
        "\n",
        "# price > ma200 = bullish\n",
        "df['bias_trend'] = np.where(df.Close.values >= df.MA200, 1, 0) \n",
        "\n",
        "df['body_candles'] = df.Open - df.Close\n",
        "df['high_low'] = df.High - df.Low\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "def norm_features(x):\n",
        "  return (x - x.mean()) / (x.max() - x.min())\n",
        "\n",
        "# normalize\n",
        "df['Returns'] = norm_features(df.Returns)\n",
        "df['Volume'] = norm_features(df.Volume)\n",
        "df['body_candles'] = norm_features(df.body_candles)\n",
        "df['high_low'] = norm_features(df.high_low)\n",
        "\n",
        "df['Target'] = np.ones((len(df)))\n",
        "df['Target'].loc[df.Returns > df.Returns.quantile(.80)] = 2\n",
        "df['Target'].loc[df.Returns < df.Returns.quantile(.20)] = 0\n",
        "\n",
        "df.drop(['Date', 'Time', 'Open', 'High', 'Low', 'Close', 'Returns', 'MA35', 'MA70', 'MA200'], axis=1, inplace=True)\n",
        "\n",
        "X = df.drop('Target', axis=1).values\n",
        "Y = df.Target.values\n",
        "\n",
        "\n",
        "# convert y into categorical\n",
        "K = 3\n",
        "from keras.utils import to_categorical\n",
        "Y = to_categorical(Y, num_classes=K)\n",
        "\n",
        "# split train test\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=42)\n",
        "\n",
        "print('train shape: ', x_train.shape)\n",
        "print('test shape: ', x_test.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:190: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  self._setitem_with_indexer(indexer, value)\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train shape:  (135548, 5)\n",
            "test shape:  (58093, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sWL1Gf_3AZS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2008
        },
        "outputId": "2b4b3943-f2aa-4057-99a9-c292a983becd"
      },
      "source": [
        "# build Neural Network using Numpy\n",
        "\n",
        "def softmax(a):\n",
        "  a = np.exp(a)\n",
        "  return a / a.sum(axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy(t, y):\n",
        "  return - np.mean(t * np.log(y))\n",
        "\n",
        "def model_score(t, y):\n",
        "  return np.round(np.mean(t == y)*100, 2)\n",
        "\n",
        "def predict(y):\n",
        "  return np.argmax(y, axis=1)\n",
        "\n",
        "\n",
        "D = X.shape[1]\n",
        "hidden_layer = 64\n",
        "lr = 0.000001\n",
        "epochs = 1000\n",
        "\n",
        "w1 = np.random.randn(D, hidden_layer)\n",
        "b1 = np.random.randn(hidden_layer)\n",
        "w2 = np.random.randn(hidden_layer, K)\n",
        "b2 = np.random.randn(K)\n",
        "\n",
        "train_costs = []\n",
        "test_costs = []\n",
        "\n",
        "for i in range(epochs):\n",
        "  # train\n",
        "  z_train = np.tanh(x_train.dot(w1) + b1)\n",
        "  y_pred_train = softmax(z_train.dot(w2) + b2)\n",
        "  cost_train = cross_entropy(y_train, y_pred_train)\n",
        "  train_costs.append(cost_train)\n",
        "  \n",
        "  # test\n",
        "  z_test = np.tanh(x_test.dot(w1) + b1)\n",
        "  y_pred_test = softmax(z_test.dot(w2) + b2)\n",
        "  cost_test = cross_entropy(y_test, y_pred_test)\n",
        "  test_costs.append(cost_test)\n",
        "  \n",
        "  if i % 10 == 0:\n",
        "    print(i, 'cost: ', cost_train)\n",
        "  \n",
        "  delta = y_train - y_pred_train\n",
        "  w2 += z_train.T.dot(delta) * lr\n",
        "  b2 += np.sum(delta) * lr\n",
        "  \n",
        "  dz = delta.dot(w2.T) * (1 - np.power(z_train, 2))\n",
        "  \n",
        "  w1 += x_train.T.dot(dz) * lr\n",
        "  b1 += np.sum(dz) * lr\n",
        "  \n",
        "  \n",
        "plt.plot(train_costs)\n",
        "plt.plot(test_costs)\n",
        "plt.show()\n",
        "\n",
        "print('Train score: ', model_score(predict(y_train), predict(y_pred_train)))\n",
        "print('Test score: ', model_score(predict(y_test), predict(y_pred_test)))\n",
        "  \n",
        "  \n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 cost:  2.06118159979107\n",
            "10 cost:  0.3626930417471763\n",
            "20 cost:  0.33585621023697093\n",
            "30 cost:  0.330687548153281\n",
            "40 cost:  0.3312373597653231\n",
            "50 cost:  0.3303519969972137\n",
            "60 cost:  0.3266365864454988\n",
            "70 cost:  0.32364459351746805\n",
            "80 cost:  0.32135284169071127\n",
            "90 cost:  0.31958719420149323\n",
            "100 cost:  0.31820009475437305\n",
            "110 cost:  0.31706781451848615\n",
            "120 cost:  0.3160961192138791\n",
            "130 cost:  0.315225422618304\n",
            "140 cost:  0.3144248793135345\n",
            "150 cost:  0.3136802963288837\n",
            "160 cost:  0.3129846740923288\n",
            "170 cost:  0.3123336160287905\n",
            "180 cost:  0.31172370281991796\n",
            "190 cost:  0.311152004205465\n",
            "200 cost:  0.31061591927478543\n",
            "210 cost:  0.31011310177403406\n",
            "220 cost:  0.30964141234874815\n",
            "230 cost:  0.30919888411745045\n",
            "240 cost:  0.30878369698285724\n",
            "250 cost:  0.30839415814967575\n",
            "260 cost:  0.3080286871274164\n",
            "270 cost:  0.307685803992772\n",
            "280 cost:  0.3073641200325589\n",
            "290 cost:  0.3070623301356621\n",
            "300 cost:  0.3067792064796393\n",
            "310 cost:  0.30651359318448695\n",
            "320 cost:  0.3062644016967079\n",
            "330 cost:  0.30603060673149324\n",
            "340 cost:  0.30581124264696175\n",
            "350 cost:  0.30560540015740056\n",
            "360 cost:  0.3054122233161812\n",
            "370 cost:  0.30523090671627395\n",
            "380 cost:  0.3050606928689867\n",
            "390 cost:  0.3049008697310671\n",
            "400 cost:  0.3047507683575577\n",
            "410 cost:  0.30460976066339696\n",
            "420 cost:  0.3044772572811342\n",
            "430 cost:  0.3043527055055651\n",
            "440 cost:  0.3042355873187676\n",
            "450 cost:  0.30412541749107913\n",
            "460 cost:  0.3040217417551069\n",
            "470 cost:  0.3039241350509759\n",
            "480 cost:  0.30383219984177817\n",
            "490 cost:  0.30374556449865486\n",
            "500 cost:  0.30366388175517006\n",
            "510 cost:  0.303586827230689\n",
            "520 cost:  0.30351409802238755\n",
            "530 cost:  0.30344541136534353\n",
            "540 cost:  0.3033805033599331\n",
            "550 cost:  0.30331912776548453\n",
            "560 cost:  0.30326105485888527\n",
            "570 cost:  0.30320607035657465\n",
            "580 cost:  0.30315397439812247\n",
            "590 cost:  0.3031045805893933\n",
            "600 cost:  0.3030577151031265\n",
            "610 cost:  0.30301321583463103\n",
            "620 cost:  0.3029709316101983\n",
            "630 cost:  0.302930721445778\n",
            "640 cost:  0.3028924538534289\n",
            "650 cost:  0.3028560061930571\n",
            "660 cost:  0.30282126406697524\n",
            "670 cost:  0.3027881207548609\n",
            "680 cost:  0.3027564766867493\n",
            "690 cost:  0.30272623895176964\n",
            "700 cost:  0.3026973208404184\n",
            "710 cost:  0.30266964141825053\n",
            "720 cost:  0.3026431251289678\n",
            "730 cost:  0.30261770142498134\n",
            "740 cost:  0.30259330442362486\n",
            "750 cost:  0.30256987258729545\n",
            "760 cost:  0.3025473484258967\n",
            "770 cost:  0.302525678220058\n",
            "780 cost:  0.3025048117636921\n",
            "790 cost:  0.30248470212455136\n",
            "800 cost:  0.3024653054215214\n",
            "810 cost:  0.3024465806174785\n",
            "820 cost:  0.3024284893266143\n",
            "830 cost:  0.30241099563520685\n",
            "840 cost:  0.3023940659348823\n",
            "850 cost:  0.3023776687674852\n",
            "860 cost:  0.30236177468072956\n",
            "870 cost:  0.30234635609386884\n",
            "880 cost:  0.30233138717266583\n",
            "890 cost:  0.30231684371300943\n",
            "900 cost:  0.3023027030325585\n",
            "910 cost:  0.3022889438698442\n",
            "920 cost:  0.30227554629030406\n",
            "930 cost:  0.30226249159875285\n",
            "940 cost:  0.3022497622578393\n",
            "950 cost:  0.3022373418120629\n",
            "960 cost:  0.30222521481696063\n",
            "970 cost:  0.3022133667730984\n",
            "980 cost:  0.3022017840645315\n",
            "990 cost:  0.3021904539014205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEBCAYAAACXArmGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train score:  0.6028417977395462\n",
            "Test score:  0.6015871103231026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k69bq8Rp54WW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2244
        },
        "outputId": "eb6dc45f-73c6-413a-af04-938210d66313"
      },
      "source": [
        "# lastest weight\n",
        "print('w1', w1)\n",
        "print('b1', b1)\n",
        "print('w2', w2)\n",
        "print('b2', b2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w1 [[ 1.79371087  1.07077856 -0.21281622  0.26305711  0.44667068  1.37783456\n",
            "   1.18006499  1.16949117  0.61277185  0.38676648  0.6161135   2.54637027\n",
            "  -0.01445924  0.34764655 -0.70405338  0.28392768  0.76154265  0.072615\n",
            "  -3.04831553 -1.09185298 -1.09718712 -0.10330902 -0.80162761  0.51508477\n",
            "  -0.24680124  0.49751665  0.88780877  1.82636859  0.15311879  0.24152141\n",
            "   0.60171631  0.78126713 -0.83873229 -0.33003845  1.41240874  0.26264324\n",
            "  -0.44137611 -0.0383305   0.85721117 -0.30077185  0.48688588 -0.63727715\n",
            "  -0.37198376  2.77823126 -0.66832923 -1.62387189 -0.11025681  1.79932871\n",
            "   0.24534953 -0.66939301 -0.44475782 -0.61951508  0.52200235 -0.77551826\n",
            "  -0.13129916  2.51021502  1.17382059  0.45243578 -1.45901688  0.34200952\n",
            "  -2.33578347  0.01898834 -1.56682751 -2.67837392]\n",
            " [-1.93274691 -0.09147809  0.16235576  0.00496338 -2.02306294  2.46806424\n",
            "   0.86541561  0.88041296  0.49096671 -1.28932098  0.49491711  1.23022623\n",
            "   0.60933804  1.60901616  0.72442038  0.88605031 -0.64904644 -0.5955547\n",
            "   0.35542835 -0.2851915   0.18160376 -3.9052931  -0.55567171  0.21025418\n",
            "   0.23989144  0.78428561 -1.23844794  1.11131622 -0.02756755  2.38205198\n",
            "  -0.9123967   1.62614108  1.29069269 -2.24523338 -0.39813476 -0.8821322\n",
            "  -1.56508494 -1.89870141  1.81272825  1.05396569  0.80642012  1.91197455\n",
            "  -0.20784923  0.74302118  0.87375976  0.70428549  0.04840665  1.68902594\n",
            "   0.68385405  0.63679908 -0.79032949  0.04202327  0.51046594 -0.46943924\n",
            "  -0.52975819 -0.28618436 -0.09937769 -0.47201525  2.32518076 -0.87605386\n",
            "  -0.62656141 -1.11987304  0.82114666  0.49158831]\n",
            " [ 0.47944655 -0.17218802  0.12901905 -1.06868959  0.58507517 -0.40741838\n",
            "   0.05555152 -1.61592227  0.16024878 -0.59427197 -0.92714651  1.92972792\n",
            "  -0.87544176 -0.27504203 -1.4259677   1.65225379 -0.89650424  0.51929395\n",
            "  -0.13394055 -0.61522883 -1.17981359  1.23216233 -0.8045857  -1.03392987\n",
            "  -0.30573187  1.15940575  0.32184289  0.79261967 -0.45007563  0.11195522\n",
            "  -0.76414381  0.48333614 -1.12612326  0.27662606  0.97644263 -1.66338011\n",
            "   1.20181439 -0.80051018  0.36701307  0.17905448 -0.52317864  0.08941423\n",
            "   0.28305481  1.20006382  0.24360245  2.22109435 -0.54471104 -0.93224845\n",
            "   0.56184379 -0.11754413 -0.2392548   0.8381716   1.43799317  0.45469536\n",
            "   0.33604539 -1.5487484  -1.11250846 -2.00353316 -1.46693411 -1.20626158\n",
            "  -1.61374795 -0.00429033  0.8766914  -0.06967269]\n",
            " [-1.44926857  3.16545424 -1.52764554 -0.51604777  0.57242695 -1.0985556\n",
            "   1.82350352  0.99967061 -1.54660771 -1.52038842 -1.4048922  -0.19944789\n",
            "  -0.85820769  0.725677   -1.51983049 -1.12421408 -0.60801025 -0.66252043\n",
            "  -0.4739048   0.17067297  0.11417908  0.15671051  0.86238911 -1.45818089\n",
            "   1.39352406  0.69633121 -1.56817309  0.65338045 -0.06120225 -2.01216427\n",
            "   0.55833951 -0.90374773  1.96634572 -0.44266682 -1.00508598 -0.97445435\n",
            "   0.1659177   0.38781585  0.72280996  1.63299535  0.17993643 -0.92920758\n",
            "  -0.15143826 -1.24754563  1.13668762 -1.67821747 -1.16305663 -1.1841836\n",
            "   1.27868333 -0.36908205 -0.91846927 -0.04069218 -0.43463536  1.67875671\n",
            "  -1.19449354 -1.16037387  0.25518049 -1.65067188  2.10282831 -1.41211395\n",
            "  -0.29788033 -2.48780382  1.39824148 -0.99009605]\n",
            " [-0.2178985  -1.42599906  0.11473141  0.5045477  -0.79721365 -0.44018611\n",
            "   0.37614948 -0.98960954 -0.16702915 -0.32944258  0.14468757 -0.35576709\n",
            "   0.47266803 -1.18110122 -0.50019585 -0.34929049  0.15680861 -1.91173546\n",
            "  -3.59358873 -0.78614458 -0.00782692 -0.22530473  0.90612565  0.1370692\n",
            "   0.21864447 -0.59484942 -0.26379377 -0.02677641 -0.68257111 -0.77997107\n",
            "   0.61348266  0.94587697 -0.65948075  0.74212183  1.00295976  0.70956515\n",
            "   0.69676003  1.353009   -1.51733807  0.54382158  0.98050446  1.45474009\n",
            "  -0.96014042  0.43614403 -1.09296719  1.29935535 -1.93640394 -0.07588307\n",
            "   0.52340023 -2.06238422 -1.37511823  1.2017655   0.6179318   1.71758284\n",
            "   0.83636363 -1.86002226 -0.85693443  0.76012213  0.72387327  0.29890971\n",
            "  -1.61344904 -1.38815566 -0.62034324  1.22800033]]\n",
            "b1 [-0.51196875  0.89300063 -0.57403825  1.13262378  2.61189934  1.43915626\n",
            " -0.47580912  1.58024002 -0.25541323  0.02714064  1.74381349 -0.08486515\n",
            " -0.33308438 -0.41655298  1.03962882  1.00809498  1.16007945  1.39236579\n",
            " -0.60035958  0.14090536 -0.21142924  1.3612445  -0.46389305 -1.38680126\n",
            "  1.20031002 -0.64858418 -0.16807306  0.32746581 -0.76360444  0.43045544\n",
            "  0.18813759  0.86392818 -1.50464465 -0.77488245 -0.35295108  0.6655378\n",
            "  2.21974335  1.47221126  0.33730283 -0.11556927  0.67832556 -1.20477227\n",
            " -1.36616976 -1.34422418 -0.29030491  0.29055849  0.32959172  0.83648383\n",
            "  0.4488787  -0.36230096  0.86508178 -1.3352634   1.59254093 -0.90199315\n",
            "  1.25096288  1.12772537 -0.93363618 -1.21071921 -0.28214814  0.56764985\n",
            " -1.04901664 -2.22169854  1.40769309  0.73353623]\n",
            "w2 [[-0.76964997 -0.29898672  0.16680247]\n",
            " [ 0.85487531  1.04351585 -2.2897218 ]\n",
            " [-0.91352495 -0.31506665 -0.07862045]\n",
            " [ 0.73791374  1.08033231  1.83960452]\n",
            " [-1.27802085 -0.07403578  0.14373574]\n",
            " [-0.4969677   0.45453176  1.26096119]\n",
            " [-0.76228663  0.82466781  1.60135614]\n",
            " [-2.0755155  -2.00810802 -0.10790622]\n",
            " [ 0.42307485  0.23931579 -0.15690049]\n",
            " [ 0.25643884 -2.78631875  0.66424294]\n",
            " [ 1.82941129  0.49338485  0.09558789]\n",
            " [-0.08590641 -0.13765959  0.21866967]\n",
            " [-0.11740032  0.38283914  0.68226399]\n",
            " [-1.67667777  0.84768445  0.13871092]\n",
            " [-0.38829311 -0.85845596  0.28746155]\n",
            " [-1.31553492  0.07144923 -2.41708396]\n",
            " [ 0.06588545 -0.50974361  0.16721055]\n",
            " [-1.4165509  -0.47564984 -0.8039059 ]\n",
            " [-0.07099652  1.46967198 -0.82560832]\n",
            " [ 0.81997092 -0.27238998  0.75527752]\n",
            " [ 1.5974051  -0.30527956  0.8667026 ]\n",
            " [-0.00620835 -0.83581478 -0.09667091]\n",
            " [-1.24797503 -0.16131827  0.15663627]\n",
            " [-0.51883449  0.15205741  0.37913611]\n",
            " [-0.34787648 -2.54904834 -2.15216767]\n",
            " [-0.12477256 -1.04569228 -0.1762978 ]\n",
            " [-0.37929415  0.05500429 -0.95163652]\n",
            " [-0.52925454 -0.8352234   0.68941027]\n",
            " [-0.243053    1.643222    0.98205546]\n",
            " [-1.37840576 -0.0514621   1.05656414]\n",
            " [-0.33030278  0.85453177 -0.26913971]\n",
            " [ 0.61963051 -1.19373084 -2.57731506]\n",
            " [-1.08717004 -2.50817696 -1.56915174]\n",
            " [-0.94394246 -1.91514712  0.2614282 ]\n",
            " [ 0.60499647 -1.41835692 -0.94170965]\n",
            " [-0.745423    0.84843257 -0.54382967]\n",
            " [-1.07257564  0.04765808  0.34933916]\n",
            " [-1.86327663  0.86795444  1.89702782]\n",
            " [ 0.2443414  -0.03711487  1.09452997]\n",
            " [ 0.47309357  0.21037629  2.06374545]\n",
            " [ 0.66228866 -0.13149104  1.31225313]\n",
            " [ 1.64973415  1.34139632 -1.89109143]\n",
            " [ 0.41023225  0.60434765 -0.03649099]\n",
            " [-0.70959159  0.18379902 -0.48152443]\n",
            " [-0.79259201 -0.37247469 -0.22364463]\n",
            " [-3.11868976 -1.14168663 -0.16735252]\n",
            " [-0.63742933  1.64050583 -0.20022323]\n",
            " [ 0.09417758  0.72302436  0.89415865]\n",
            " [ 0.95602382 -2.22421775 -0.54800924]\n",
            " [-1.37284729 -0.63851704  0.84284027]\n",
            " [-0.19381381  2.01833332 -0.10301544]\n",
            " [ 1.09701663 -1.30514652  0.19736734]\n",
            " [-0.5518281   1.4550554  -0.15854436]\n",
            " [-1.64770718  0.57143254  0.66319658]\n",
            " [ 0.31651849 -0.10071949 -0.74020663]\n",
            " [ 0.42243056  0.18822707 -1.40071143]\n",
            " [-0.12424722  0.57902965 -0.2837236 ]\n",
            " [-0.93169697  1.6824802   1.7078711 ]\n",
            " [ 0.31699259 -1.18324703  0.30329581]\n",
            " [ 2.17198051 -1.3942577  -0.04601898]\n",
            " [ 1.00717603  1.88489882 -1.1045202 ]\n",
            " [-0.57223875  0.31602401  0.46312748]\n",
            " [-0.88248218  1.29372947  0.66713518]\n",
            " [ 0.63324238 -0.57483644 -1.09506872]]\n",
            "b2 [-0.56253806  1.44924007  0.52113609]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AIAOqSX4hAW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3536
        },
        "outputId": "5abdd9c6-14d2-4c4f-8bb2-2c9e6bf1ad61"
      },
      "source": [
        "# use keras for comparision\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(patience=10, monitor='loss', mode='min')\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(64, input_shape=(X.shape[1],), activation=tf.nn.tanh),\n",
        "    Dense(K, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.000001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=1000, callbacks=[early_stop], validation_split=0.3, validation_data=(x_test, y_test), batch_size=32, verbose=1)\n",
        "model.evaluate(x_final, y_final)\n",
        "\n",
        "def history_plot(histories, key='acc'):\n",
        "    plt.figure(figsize=(16,10))\n",
        "\n",
        "    for name, history in histories:\n",
        "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
        "                       '--', label=name.title()+' Val')\n",
        "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
        "                 label=name.title()+' Train')\n",
        "\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel(key.replace('_',' ').title())\n",
        "        plt.legend()\n",
        "\n",
        "        plt.xlim([0,max(history.epoch)])\n",
        "        \n",
        "history_plot(('model', history))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 135548 samples, validate on 58093 samples\n",
            "Epoch 1/1000\n",
            "135548/135548 [==============================] - 18s 135us/sample - loss: 1.1115 - acc: 0.3748 - val_loss: 1.0907 - val_acc: 0.3884\n",
            "Epoch 2/1000\n",
            "135548/135548 [==============================] - 18s 132us/sample - loss: 1.0720 - acc: 0.5288 - val_loss: 1.0555 - val_acc: 0.5499\n",
            "Epoch 3/1000\n",
            "135548/135548 [==============================] - 18s 132us/sample - loss: 1.0399 - acc: 0.5910 - val_loss: 1.0275 - val_acc: 0.5956\n",
            "Epoch 4/1000\n",
            "135548/135548 [==============================] - 18s 133us/sample - loss: 1.0148 - acc: 0.5983 - val_loss: 1.0062 - val_acc: 0.5979\n",
            "Epoch 5/1000\n",
            "135548/135548 [==============================] - 18s 132us/sample - loss: 0.9963 - acc: 0.5993 - val_loss: 0.9910 - val_acc: 0.5983\n",
            "Epoch 6/1000\n",
            "135548/135548 [==============================] - 18s 132us/sample - loss: 0.9834 - acc: 0.5997 - val_loss: 0.9807 - val_acc: 0.5988\n",
            "Epoch 7/1000\n",
            "135548/135548 [==============================] - 18s 132us/sample - loss: 0.9748 - acc: 0.6001 - val_loss: 0.9739 - val_acc: 0.5990\n",
            "Epoch 8/1000\n",
            "135548/135548 [==============================] - 18s 132us/sample - loss: 0.9691 - acc: 0.6002 - val_loss: 0.9693 - val_acc: 0.5990\n",
            "Epoch 9/1000\n",
            "135548/135548 [==============================] - 18s 132us/sample - loss: 0.9651 - acc: 0.6003 - val_loss: 0.9660 - val_acc: 0.5990\n",
            "Epoch 10/1000\n",
            "135548/135548 [==============================] - 18s 132us/sample - loss: 0.9621 - acc: 0.6003 - val_loss: 0.9633 - val_acc: 0.5989\n",
            "Epoch 11/1000\n",
            "135548/135548 [==============================] - 18s 135us/sample - loss: 0.9597 - acc: 0.6003 - val_loss: 0.9612 - val_acc: 0.5990\n",
            "Epoch 12/1000\n",
            "135548/135548 [==============================] - 18s 133us/sample - loss: 0.9577 - acc: 0.6003 - val_loss: 0.9592 - val_acc: 0.5992\n",
            "Epoch 13/1000\n",
            "135548/135548 [==============================] - 18s 132us/sample - loss: 0.9559 - acc: 0.6003 - val_loss: 0.9575 - val_acc: 0.5992\n",
            "Epoch 14/1000\n",
            "135548/135548 [==============================] - 18s 132us/sample - loss: 0.9543 - acc: 0.6003 - val_loss: 0.9559 - val_acc: 0.5992\n",
            "Epoch 15/1000\n",
            "135548/135548 [==============================] - 18s 134us/sample - loss: 0.9528 - acc: 0.6003 - val_loss: 0.9544 - val_acc: 0.5992\n",
            "Epoch 16/1000\n",
            "135548/135548 [==============================] - 18s 133us/sample - loss: 0.9514 - acc: 0.6003 - val_loss: 0.9530 - val_acc: 0.5992\n",
            "Epoch 17/1000\n",
            "135548/135548 [==============================] - 18s 132us/sample - loss: 0.9502 - acc: 0.6004 - val_loss: 0.9518 - val_acc: 0.5992\n",
            "Epoch 18/1000\n",
            "135548/135548 [==============================] - 18s 132us/sample - loss: 0.9490 - acc: 0.6004 - val_loss: 0.9506 - val_acc: 0.5992\n",
            "Epoch 19/1000\n",
            "135548/135548 [==============================] - 18s 132us/sample - loss: 0.9479 - acc: 0.6004 - val_loss: 0.9495 - val_acc: 0.5992\n",
            "Epoch 20/1000\n",
            "135548/135548 [==============================] - 18s 132us/sample - loss: 0.9469 - acc: 0.6004 - val_loss: 0.9485 - val_acc: 0.5993\n",
            "Epoch 21/1000\n",
            "135548/135548 [==============================] - 18s 132us/sample - loss: 0.9460 - acc: 0.6004 - val_loss: 0.9475 - val_acc: 0.5993\n",
            "Epoch 22/1000\n",
            "135548/135548 [==============================] - 18s 132us/sample - loss: 0.9451 - acc: 0.6004 - val_loss: 0.9467 - val_acc: 0.5993\n",
            "Epoch 23/1000\n",
            "135548/135548 [==============================] - 18s 132us/sample - loss: 0.9443 - acc: 0.6004 - val_loss: 0.9458 - val_acc: 0.5993\n",
            "Epoch 24/1000\n",
            "135548/135548 [==============================] - 18s 132us/sample - loss: 0.9436 - acc: 0.6004 - val_loss: 0.9450 - val_acc: 0.5993\n",
            "Epoch 25/1000\n",
            "135548/135548 [==============================] - 18s 132us/sample - loss: 0.9428 - acc: 0.6004 - val_loss: 0.9443 - val_acc: 0.5993\n",
            "Epoch 26/1000\n",
            "135548/135548 [==============================] - 18s 133us/sample - loss: 0.9422 - acc: 0.6004 - val_loss: 0.9436 - val_acc: 0.5993\n",
            "Epoch 27/1000\n",
            "135548/135548 [==============================] - 18s 132us/sample - loss: 0.9415 - acc: 0.6004 - val_loss: 0.9429 - val_acc: 0.5993\n",
            "Epoch 28/1000\n",
            "135548/135548 [==============================] - 18s 133us/sample - loss: 0.9409 - acc: 0.6005 - val_loss: 0.9423 - val_acc: 0.5993\n",
            "Epoch 29/1000\n",
            "135548/135548 [==============================] - 18s 131us/sample - loss: 0.9404 - acc: 0.6005 - val_loss: 0.9417 - val_acc: 0.5993\n",
            "Epoch 30/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9398 - acc: 0.6005 - val_loss: 0.9412 - val_acc: 0.5993\n",
            "Epoch 31/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9393 - acc: 0.6005 - val_loss: 0.9406 - val_acc: 0.5993\n",
            "Epoch 32/1000\n",
            "135548/135548 [==============================] - 17s 128us/sample - loss: 0.9388 - acc: 0.6005 - val_loss: 0.9401 - val_acc: 0.5993\n",
            "Epoch 33/1000\n",
            "135548/135548 [==============================] - 17s 128us/sample - loss: 0.9384 - acc: 0.6005 - val_loss: 0.9396 - val_acc: 0.5993\n",
            "Epoch 34/1000\n",
            "135548/135548 [==============================] - 18s 129us/sample - loss: 0.9379 - acc: 0.6005 - val_loss: 0.9391 - val_acc: 0.5993\n",
            "Epoch 35/1000\n",
            "135548/135548 [==============================] - 17s 129us/sample - loss: 0.9375 - acc: 0.6005 - val_loss: 0.9387 - val_acc: 0.5993\n",
            "Epoch 36/1000\n",
            "135548/135548 [==============================] - 18s 130us/sample - loss: 0.9370 - acc: 0.6005 - val_loss: 0.9382 - val_acc: 0.5993\n",
            "Epoch 37/1000\n",
            "135548/135548 [==============================] - 18s 130us/sample - loss: 0.9366 - acc: 0.6005 - val_loss: 0.9378 - val_acc: 0.5993\n",
            "Epoch 38/1000\n",
            "135548/135548 [==============================] - 18s 130us/sample - loss: 0.9362 - acc: 0.6006 - val_loss: 0.9374 - val_acc: 0.5993\n",
            "Epoch 39/1000\n",
            "135548/135548 [==============================] - 18s 130us/sample - loss: 0.9358 - acc: 0.6006 - val_loss: 0.9370 - val_acc: 0.5993\n",
            "Epoch 40/1000\n",
            "135548/135548 [==============================] - 18s 129us/sample - loss: 0.9355 - acc: 0.6006 - val_loss: 0.9366 - val_acc: 0.5994\n",
            "Epoch 41/1000\n",
            "135548/135548 [==============================] - 17s 129us/sample - loss: 0.9351 - acc: 0.6006 - val_loss: 0.9362 - val_acc: 0.5994\n",
            "Epoch 42/1000\n",
            "135548/135548 [==============================] - 17s 129us/sample - loss: 0.9347 - acc: 0.6007 - val_loss: 0.9358 - val_acc: 0.5994\n",
            "Epoch 43/1000\n",
            "135548/135548 [==============================] - 17s 128us/sample - loss: 0.9344 - acc: 0.6007 - val_loss: 0.9354 - val_acc: 0.5994\n",
            "Epoch 44/1000\n",
            "135548/135548 [==============================] - 17s 129us/sample - loss: 0.9340 - acc: 0.6007 - val_loss: 0.9350 - val_acc: 0.5995\n",
            "Epoch 45/1000\n",
            "135548/135548 [==============================] - 17s 128us/sample - loss: 0.9337 - acc: 0.6007 - val_loss: 0.9347 - val_acc: 0.5995\n",
            "Epoch 46/1000\n",
            "135548/135548 [==============================] - 18s 131us/sample - loss: 0.9334 - acc: 0.6007 - val_loss: 0.9343 - val_acc: 0.5994\n",
            "Epoch 47/1000\n",
            "135548/135548 [==============================] - 17s 129us/sample - loss: 0.9330 - acc: 0.6007 - val_loss: 0.9340 - val_acc: 0.5995\n",
            "Epoch 48/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9327 - acc: 0.6008 - val_loss: 0.9336 - val_acc: 0.5995\n",
            "Epoch 49/1000\n",
            "135548/135548 [==============================] - 17s 128us/sample - loss: 0.9324 - acc: 0.6008 - val_loss: 0.9333 - val_acc: 0.5995\n",
            "Epoch 50/1000\n",
            "135548/135548 [==============================] - 17s 129us/sample - loss: 0.9321 - acc: 0.6008 - val_loss: 0.9330 - val_acc: 0.5995\n",
            "Epoch 51/1000\n",
            "135548/135548 [==============================] - 17s 128us/sample - loss: 0.9317 - acc: 0.6008 - val_loss: 0.9326 - val_acc: 0.5995\n",
            "Epoch 52/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9314 - acc: 0.6009 - val_loss: 0.9323 - val_acc: 0.5995\n",
            "Epoch 53/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9311 - acc: 0.6008 - val_loss: 0.9320 - val_acc: 0.5995\n",
            "Epoch 54/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9308 - acc: 0.6008 - val_loss: 0.9316 - val_acc: 0.5996\n",
            "Epoch 55/1000\n",
            "135548/135548 [==============================] - 18s 129us/sample - loss: 0.9305 - acc: 0.6009 - val_loss: 0.9313 - val_acc: 0.5996\n",
            "Epoch 56/1000\n",
            "135548/135548 [==============================] - 18s 131us/sample - loss: 0.9302 - acc: 0.6008 - val_loss: 0.9310 - val_acc: 0.5996\n",
            "Epoch 57/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9299 - acc: 0.6008 - val_loss: 0.9307 - val_acc: 0.5996\n",
            "Epoch 58/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9296 - acc: 0.6009 - val_loss: 0.9304 - val_acc: 0.5996\n",
            "Epoch 59/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9293 - acc: 0.6009 - val_loss: 0.9301 - val_acc: 0.5996\n",
            "Epoch 60/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9291 - acc: 0.6009 - val_loss: 0.9298 - val_acc: 0.5997\n",
            "Epoch 61/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9288 - acc: 0.6009 - val_loss: 0.9295 - val_acc: 0.5997\n",
            "Epoch 62/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9285 - acc: 0.6009 - val_loss: 0.9292 - val_acc: 0.5997\n",
            "Epoch 63/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9282 - acc: 0.6009 - val_loss: 0.9289 - val_acc: 0.5997\n",
            "Epoch 64/1000\n",
            "135548/135548 [==============================] - 18s 131us/sample - loss: 0.9279 - acc: 0.6010 - val_loss: 0.9286 - val_acc: 0.5998\n",
            "Epoch 65/1000\n",
            "135548/135548 [==============================] - 17s 129us/sample - loss: 0.9276 - acc: 0.6010 - val_loss: 0.9283 - val_acc: 0.5998\n",
            "Epoch 66/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9274 - acc: 0.6011 - val_loss: 0.9280 - val_acc: 0.5998\n",
            "Epoch 67/1000\n",
            "135548/135548 [==============================] - 17s 128us/sample - loss: 0.9271 - acc: 0.6011 - val_loss: 0.9277 - val_acc: 0.5998\n",
            "Epoch 68/1000\n",
            "135548/135548 [==============================] - 17s 129us/sample - loss: 0.9268 - acc: 0.6012 - val_loss: 0.9274 - val_acc: 0.6000\n",
            "Epoch 69/1000\n",
            "135548/135548 [==============================] - 17s 128us/sample - loss: 0.9265 - acc: 0.6012 - val_loss: 0.9271 - val_acc: 0.6001\n",
            "Epoch 70/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9263 - acc: 0.6012 - val_loss: 0.9268 - val_acc: 0.6001\n",
            "Epoch 71/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9260 - acc: 0.6012 - val_loss: 0.9266 - val_acc: 0.6002\n",
            "Epoch 72/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9257 - acc: 0.6013 - val_loss: 0.9263 - val_acc: 0.6002\n",
            "Epoch 73/1000\n",
            "135548/135548 [==============================] - 17s 126us/sample - loss: 0.9255 - acc: 0.6013 - val_loss: 0.9260 - val_acc: 0.6002\n",
            "Epoch 74/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9252 - acc: 0.6013 - val_loss: 0.9257 - val_acc: 0.6003\n",
            "Epoch 75/1000\n",
            "135548/135548 [==============================] - 17s 126us/sample - loss: 0.9250 - acc: 0.6014 - val_loss: 0.9255 - val_acc: 0.6003\n",
            "Epoch 76/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9247 - acc: 0.6014 - val_loss: 0.9252 - val_acc: 0.6003\n",
            "Epoch 77/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9244 - acc: 0.6014 - val_loss: 0.9249 - val_acc: 0.6003\n",
            "Epoch 78/1000\n",
            "135548/135548 [==============================] - 17s 126us/sample - loss: 0.9242 - acc: 0.6015 - val_loss: 0.9246 - val_acc: 0.6004\n",
            "Epoch 79/1000\n",
            "135548/135548 [==============================] - 17s 126us/sample - loss: 0.9239 - acc: 0.6015 - val_loss: 0.9244 - val_acc: 0.6004\n",
            "Epoch 80/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9237 - acc: 0.6015 - val_loss: 0.9241 - val_acc: 0.6005\n",
            "Epoch 81/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9234 - acc: 0.6016 - val_loss: 0.9238 - val_acc: 0.6004\n",
            "Epoch 82/1000\n",
            "135548/135548 [==============================] - 18s 129us/sample - loss: 0.9232 - acc: 0.6016 - val_loss: 0.9236 - val_acc: 0.6004\n",
            "Epoch 83/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9229 - acc: 0.6017 - val_loss: 0.9233 - val_acc: 0.6005\n",
            "Epoch 84/1000\n",
            "135548/135548 [==============================] - 17s 126us/sample - loss: 0.9227 - acc: 0.6017 - val_loss: 0.9231 - val_acc: 0.6005\n",
            "Epoch 85/1000\n",
            "135548/135548 [==============================] - 17s 128us/sample - loss: 0.9225 - acc: 0.6017 - val_loss: 0.9228 - val_acc: 0.6006\n",
            "Epoch 86/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9222 - acc: 0.6017 - val_loss: 0.9226 - val_acc: 0.6006\n",
            "Epoch 87/1000\n",
            "135548/135548 [==============================] - 17s 126us/sample - loss: 0.9220 - acc: 0.6018 - val_loss: 0.9223 - val_acc: 0.6006\n",
            "Epoch 88/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9217 - acc: 0.6019 - val_loss: 0.9221 - val_acc: 0.6006\n",
            "Epoch 89/1000\n",
            "135548/135548 [==============================] - 17s 126us/sample - loss: 0.9215 - acc: 0.6020 - val_loss: 0.9218 - val_acc: 0.6006\n",
            "Epoch 90/1000\n",
            "135548/135548 [==============================] - 17s 126us/sample - loss: 0.9213 - acc: 0.6020 - val_loss: 0.9216 - val_acc: 0.6006\n",
            "Epoch 91/1000\n",
            "135548/135548 [==============================] - 17s 126us/sample - loss: 0.9211 - acc: 0.6021 - val_loss: 0.9213 - val_acc: 0.6006\n",
            "Epoch 92/1000\n",
            "135548/135548 [==============================] - 17s 126us/sample - loss: 0.9208 - acc: 0.6022 - val_loss: 0.9211 - val_acc: 0.6008\n",
            "Epoch 93/1000\n",
            "135548/135548 [==============================] - 17s 126us/sample - loss: 0.9206 - acc: 0.6022 - val_loss: 0.9208 - val_acc: 0.6008\n",
            "Epoch 94/1000\n",
            "135548/135548 [==============================] - 17s 126us/sample - loss: 0.9204 - acc: 0.6023 - val_loss: 0.9206 - val_acc: 0.6009\n",
            "Epoch 95/1000\n",
            "135548/135548 [==============================] - 17s 126us/sample - loss: 0.9202 - acc: 0.6023 - val_loss: 0.9204 - val_acc: 0.6010\n",
            "Epoch 96/1000\n",
            "135548/135548 [==============================] - 17s 126us/sample - loss: 0.9199 - acc: 0.6023 - val_loss: 0.9201 - val_acc: 0.6009\n",
            "Epoch 97/1000\n",
            "135548/135548 [==============================] - 17s 126us/sample - loss: 0.9197 - acc: 0.6023 - val_loss: 0.9199 - val_acc: 0.6010\n",
            "Epoch 98/1000\n",
            "135548/135548 [==============================] - 17s 126us/sample - loss: 0.9195 - acc: 0.6024 - val_loss: 0.9197 - val_acc: 0.6009\n",
            "Epoch 99/1000\n",
            "135548/135548 [==============================] - 17s 126us/sample - loss: 0.9193 - acc: 0.6023 - val_loss: 0.9195 - val_acc: 0.6010\n",
            "Epoch 100/1000\n",
            "135548/135548 [==============================] - 17s 128us/sample - loss: 0.9191 - acc: 0.6023 - val_loss: 0.9192 - val_acc: 0.6009\n",
            "Epoch 101/1000\n",
            "135548/135548 [==============================] - 17s 128us/sample - loss: 0.9189 - acc: 0.6024 - val_loss: 0.9190 - val_acc: 0.6010\n",
            "Epoch 102/1000\n",
            "135548/135548 [==============================] - 17s 127us/sample - loss: 0.9187 - acc: 0.6024 - val_loss: 0.9188 - val_acc: 0.6010\n",
            "Epoch 103/1000\n",
            "  3104/135548 [..............................] - ETA: 13s - loss: 0.9083 - acc: 0.6079"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "477HWofC7a6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}