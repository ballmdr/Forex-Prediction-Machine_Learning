{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Forex Prediction Deep Learning Pure Numpy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ballmdr/Machine-Learning/blob/master/Forex_Prediction_Deep_Learning_Pure_Numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1l52HoiFlgv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5f31118-8259-4741-bfc0-3a728af90baa"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjahuGTDFqEC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "cd0b1ab4-2a6a-4570-97d5-72ce9ca77d0a"
      },
      "source": [
        "filename = '/content/drive/My Drive/export/M30_all_EURUSD.csv'\n",
        "df = pd.read_csv(filename, names=['Date', 'Time', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
        "df['Datetime'] = pd.to_datetime(df.Date + ' ' + df.Time)\n",
        "df.set_index('Datetime', inplace=True)\n",
        "\n",
        "# Features for Momentum Strategy\n",
        "df['Returns'] = - df.Close.pct_change(-1)\n",
        "\n",
        "df['MA35'] = df.Close.rolling(35).mean()\n",
        "df['MA70'] = df.Close.rolling(70).mean()\n",
        "df['MA200'] = df.Close.rolling(200).mean()\n",
        "\n",
        "# ma35 > ma70 = bullish\n",
        "df['35_70'] = np.where(df.MA35 >= df.MA70, 1, 0)\n",
        "\n",
        "# price > ma200 = bullish\n",
        "df['bias_trend'] = np.where(df.Close.values >= df.MA200, 1, 0) \n",
        "\n",
        "df['body_candles'] = df.Open - df.Close\n",
        "df['high_low'] = df.High - df.Low\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "def norm_features(x):\n",
        "  return (x - x.mean()) / (x.max() - x.min())\n",
        "\n",
        "# normalize\n",
        "df['Returns'] = norm_features(df.Returns)\n",
        "df['Volume'] = norm_features(df.Volume)\n",
        "df['body_candles'] = norm_features(df.body_candles)\n",
        "df['high_low'] = norm_features(df.high_low)\n",
        "\n",
        "df['Target'] = np.ones((len(df)))\n",
        "df['Target'].loc[df.Returns > df.Returns.quantile(.80)] = 2\n",
        "df['Target'].loc[df.Returns < df.Returns.quantile(.20)] = 0\n",
        "\n",
        "df.drop(['Date', 'Time', 'Open', 'High', 'Low', 'Close', 'Returns', 'MA35', 'MA70', 'MA200'], axis=1, inplace=True)\n",
        "\n",
        "X = df.drop('Target', axis=1).values\n",
        "Y = df.Target.values\n",
        "\n",
        "\n",
        "# convert y into categorical\n",
        "K = 3\n",
        "from keras.utils import to_categorical\n",
        "Y = to_categorical(Y, num_classes=K)\n",
        "\n",
        "# split train test\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=42)\n",
        "\n",
        "print('train shape: ', x_train.shape)\n",
        "print('test shape: ', x_test.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:190: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  self._setitem_with_indexer(indexer, value)\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train shape:  (135548, 5)\n",
            "test shape:  (58093, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxRZgAZkFu9d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1139
        },
        "outputId": "5391cd08-a5a6-4b57-be97-50d3ff517309"
      },
      "source": [
        "def softmax(a):\n",
        "  a = np.exp(a)\n",
        "  return a / a.sum(axis=1, keepdims=True)\n",
        "\n",
        "def sigmoid(z):\n",
        "  return 1 / 1 + np.exp(-z)\n",
        "\n",
        "def cross_entropy(t, y):\n",
        "  return - np.mean(t * np.log(y))\n",
        "\n",
        "def model_score(t, y):\n",
        "  return np.mean(t == y)\n",
        "\n",
        "def predict(y):\n",
        "  return np.argmax(y, axis=1)\n",
        "\n",
        "# Deep Neural Network using Only Numpy\n",
        "lr = 0.0000001\n",
        "epochs = 1000\n",
        "\n",
        "train_costs = []\n",
        "test_costs = []\n",
        "\n",
        "hidden1 = 64\n",
        "hidden2 = 64\n",
        "hidden3 = 64\n",
        "k = 3\n",
        "\n",
        "w1 = np.random.randn(X.shape[1], hidden1)\n",
        "b1 = np.random.randn(hidden1)\n",
        "w2 = np.random.randn(hidden1, hidden2)\n",
        "b2 = np.random.randn(hidden2)\n",
        "w3 = np.random.randn(hidden2, hidden3)\n",
        "b3 = np.random.randn(hidden3)\n",
        "w4 = np.random.randn(hidden3, k)\n",
        "b4 = np.random.randn(k)\n",
        "\n",
        "for i in range(epochs):\n",
        "  #feed forward train \n",
        "  z_train1 = np.tanh(x_train.dot(w1) + b1)\n",
        "  z_train2 = np.tanh(z_train1.dot(w2) + b2)\n",
        "  z_train3 = np.tanh(z_train2.dot(w3) + b3)\n",
        "  y_pred_train = softmax(z_train3.dot(w4) + b4)\n",
        "  cost_train = cross_entropy(y_train, y_pred_train)\n",
        "  train_costs.append(cost_train)\n",
        "  \n",
        "  if i % 10 == 0:\n",
        "    print(i, ' cost: ', cost_train)\n",
        "  \n",
        "  #feed forward test\n",
        "  z_test1 = np.tanh(x_test.dot(w1) + b1)\n",
        "  z_test2 = np.tanh(z_test1.dot(w2) + b2)\n",
        "  z_test3 = np.tanh(z_test2.dot(w3) + b3)\n",
        "  y_pred_test = softmax(z_test3.dot(w4) + b4)\n",
        "  cost_test = cross_entropy(y_test, y_pred_test)\n",
        "  test_costs.append(cost_test)\n",
        "  \n",
        "  #Backpropagation (Gradient descent)\n",
        "  delta = y_train - y_pred_train\n",
        "  w4 += z_train3.T.dot(delta) * lr\n",
        "  b4 += np.sum(delta) * lr\n",
        "  \n",
        "  dz3 = delta.dot(w4.T) * (1 - np.power(z_train3, 2))\n",
        "  \n",
        "  w3 += z_train2.T.dot(dz3) * lr\n",
        "  b3 += np.sum(dz3) * lr\n",
        "  \n",
        "  dz2 = z_train3.dot(w3.T) * (1 - np.power(z_train2, 2))\n",
        "  \n",
        "  w2 += z_train1.T.dot(dz2) * lr\n",
        "  b2 += np.sum(dz2) * lr\n",
        "  \n",
        "  dz = z_train2.dot(w2.T) * (1 - np.power(z_train1, 2))\n",
        "  \n",
        "  w1 += x_train.T.dot(dz) * lr\n",
        "  b1 += np.sum(dz) * lr\n",
        "  \n",
        "  \n",
        "  \n",
        "plt.plot(train_costs)\n",
        "plt.plot(test_costs)\n",
        "plt.show()\n",
        "\n",
        "print(model_score(predict(y_test), predict(y_pred_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0  cost:  3.554146577500309\n",
            "10  cost:  1.1629438695221426\n",
            "20  cost:  0.56915242354883\n",
            "30  cost:  0.47486401234833386\n",
            "40  cost:  0.40401978898100266\n",
            "50  cost:  0.394796285811895\n",
            "60  cost:  0.36455231080680145\n",
            "70  cost:  0.3608672100486248\n",
            "80  cost:  0.4054843320232606\n",
            "90  cost:  0.3922087734280641\n",
            "100  cost:  0.37035826114578846\n",
            "110  cost:  0.33966197590660424\n",
            "120  cost:  0.3243096211059488\n",
            "130  cost:  0.32048501784083705\n",
            "140  cost:  0.31883193361682033\n",
            "150  cost:  0.31800319610583416\n",
            "160  cost:  0.3175342296635734\n",
            "170  cost:  0.31724624672081825\n",
            "180  cost:  0.3170581241203034\n",
            "190  cost:  0.31692862649153686\n",
            "200  cost:  0.31683536435252785\n",
            "210  cost:  0.3167656304637864\n",
            "220  cost:  0.3167118826434337\n",
            "230  cost:  0.316669405861054\n",
            "240  cost:  0.31663515379250795\n",
            "250  cost:  0.31660711085685034\n",
            "260  cost:  0.3165838810579945\n",
            "270  cost:  0.31656445775223213\n",
            "280  cost:  0.31654810274650597\n",
            "290  cost:  0.3165342667475754\n",
            "300  cost:  0.3165225300418701\n",
            "310  cost:  0.31651256164106406\n",
            "320  cost:  0.31650409341948943\n",
            "330  cost:  0.31649690396877755\n",
            "340  cost:  0.31649080808449853\n",
            "350  cost:  0.31648564947785135\n",
            "360  cost:  0.3164812954242008\n",
            "370  cost:  0.31647763265909234\n",
            "380  cost:  0.3164745641338193\n",
            "390  cost:  0.3164720063964796\n",
            "400  cost:  0.3164698874469628\n",
            "410  cost:  0.3164681449613844\n",
            "420  cost:  0.3164667248100621\n",
            "430  cost:  0.3164655798115477\n",
            "440  cost:  0.3164646686777304\n",
            "450  cost:  0.31646395511389913\n",
            "460  cost:  0.3164634070441417\n",
            "470  cost:  0.3164629959372244\n",
            "480  cost:  0.3164626962115467\n",
            "490  cost:  0.3164624847000759\n",
            "500  cost:  0.3164623401574044\n",
            "510  cost:  0.31646224279120977\n",
            "520  cost:  0.3164621737992384\n",
            "530  cost:  0.31646211489013465\n",
            "540  cost:  0.31646204776133785\n",
            "550  cost:  0.31646195349874845\n",
            "560  cost:  0.3164618118490341\n",
            "570  cost:  0.31646160029300274\n",
            "580  cost:  0.3164612928116445\n",
            "590  cost:  0.3164608581747589\n",
            "600  cost:  0.3164602574759106\n",
            "610  cost:  0.3164594404485662\n",
            "620  cost:  0.31645833974881926\n",
            "630  cost:  0.31645686171329723\n",
            "640  cost:  0.31645487071727063\n",
            "650  cost:  0.31645216124193004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66VTreQOFzU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use keras for comparision\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(patience=10, monitor='loss', mode='min')\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(64, input_shape=(X.shape[1],), activation=tf.nn.tanh),\n",
        "    Dense(64, activation=tf.nn.tanh),\n",
        "    Dense(64, activation=tf.nn.tanh),\n",
        "    Dense(K, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.000001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=1000, callbacks=[early_stop], validation_split=0.3, validation_data=(x_test, y_test), batch_size=32, verbose=1)\n",
        "model.evaluate(x_final, y_final)\n",
        "\n",
        "def history_plot(histories, key='acc'):\n",
        "    plt.figure(figsize=(16,10))\n",
        "\n",
        "    for name, history in histories:\n",
        "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
        "                       '--', label=name.title()+' Val')\n",
        "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
        "                 label=name.title()+' Train')\n",
        "\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel(key.replace('_',' ').title())\n",
        "        plt.legend()\n",
        "\n",
        "        plt.xlim([0,max(history.epoch)])\n",
        "        \n",
        "history_plot(('model', history))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpJyvXaIH__k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}