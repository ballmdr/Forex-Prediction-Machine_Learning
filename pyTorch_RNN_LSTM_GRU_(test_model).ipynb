{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyTorch RNN LSTM GRU (test model).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ballmdr/Forex-Prediction-Machine_Learning/blob/master/pyTorch_RNN_LSTM_GRU_(test_model).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t0fcAJQa1Eb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhUvtCRra14x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = {\n",
        "  'good': True,\n",
        "  'bad': False,\n",
        "  'happy': True,\n",
        "  'sad': False,\n",
        "  'not good': False,\n",
        "  'not bad': True,\n",
        "  'not happy': False,\n",
        "  'not sad': True,\n",
        "  'very good': True,\n",
        "  'very bad': False,\n",
        "  'very happy': True,\n",
        "  'very sad': False,\n",
        "  'i am happy': True,\n",
        "  'this is good': True,\n",
        "  'i am bad': False,\n",
        "  'this is bad': False,\n",
        "  'i am sad': False,\n",
        "  'this is sad': False,\n",
        "  'i am not happy': False,\n",
        "  'this is not good': False,\n",
        "  'i am not bad': True,\n",
        "  'this is not sad': True,\n",
        "  'i am very happy': True,\n",
        "  'this is very good': True,\n",
        "  'i am very bad': False,\n",
        "  'this is very sad': False,\n",
        "  'this is very happy': True,\n",
        "  'i am good not bad': True,\n",
        "  'this is good not bad': True,\n",
        "  'i am bad not good': False,\n",
        "  'i am good and happy': True,\n",
        "  'this is not good and not happy': False,\n",
        "  'i am not at all good': False,\n",
        "  'i am not at all bad': True,\n",
        "  'i am not at all happy': False,\n",
        "  'this is not at all sad': True,\n",
        "  'this is not at all happy': False,\n",
        "  'i am good right now': True,\n",
        "  'i am bad right now': False,\n",
        "  'this is bad right now': False,\n",
        "  'i am sad right now': False,\n",
        "  'i was good earlier': True,\n",
        "  'i was happy earlier': True,\n",
        "  'i was bad earlier': False,\n",
        "  'i was sad earlier': False,\n",
        "  'i am very bad right now': False,\n",
        "  'this is very good right now': True,\n",
        "  'this is very sad right now': False,\n",
        "  'this was bad earlier': False,\n",
        "  'this was very good earlier': True,\n",
        "  'this was very bad earlier': False,\n",
        "  'this was very happy earlier': True,\n",
        "  'this was very sad earlier': False,\n",
        "  'i was good and not bad earlier': True,\n",
        "  'i was not good and not happy earlier': False,\n",
        "  'i am not at all bad or sad right now': True,\n",
        "  'i am not at all good or happy right now': False,\n",
        "  'this was not happy and not good earlier': False,\n",
        "}\n",
        "\n",
        "test_data = {\n",
        "  'this is happy': True,\n",
        "  'i am good': True,\n",
        "  'this is not happy': False,\n",
        "  'i am not good': False,\n",
        "  'this is not bad': True,\n",
        "  'i am not sad': True,\n",
        "  'i am very good': True,\n",
        "  'this is very bad': False,\n",
        "  'i am very sad': False,\n",
        "  'this is bad not good': False,\n",
        "  'this is good and happy': True,\n",
        "  'i am not good and not happy': False,\n",
        "  'i am not at all sad': True,\n",
        "  'this is not at all good': False,\n",
        "  'this is not at all bad': True,\n",
        "  'this is good right now': True,\n",
        "  'this is sad right now': False,\n",
        "  'this is very bad right now': False,\n",
        "  'this was good earlier': True,\n",
        "  'i was not happy and not good earlier': False,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Mg_BKi53WnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def createInputs(text):\n",
        "    inputs = []\n",
        "    for w in text.split(' '):\n",
        "        v = np.zeros((vocab_size, 1))\n",
        "        v[word_to_idx[w]] = 1\n",
        "        inputs.append(v)\n",
        "        #print(v.reshape(1,-1))\n",
        "        #print('-----')\n",
        "\n",
        "    return inputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK4yKxO7a531",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = set()\n",
        "for text in train_data.keys():\n",
        "    for w in text.split(' '):\n",
        "        vocab.add(w)\n",
        "        \n",
        "vocab = list(vocab)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_idx = { w: i for i, w in enumerate(vocab) }\n",
        "idx_to_word = {i: w for i, w in enumerate(vocab) }\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "for key, val in train_data.items():\n",
        "    x_train.append(key)\n",
        "    y_train.append(int(val))\n",
        "    \n",
        "#x_test = np.array(x_test, np.float32)\n",
        "#y_train = np.array(y_train, np.float32)\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "for key, val in test_data.items():\n",
        "    x_test.append(key)\n",
        "    y_test.append(int(val))\n",
        "    \n",
        "#x_test = np.array(x_test, np.float32)\n",
        "#y_test = np.array(y_test, np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghPoip5R3hbz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fb7e15a8-d9bd-4681-dc37-93143d2b7e65"
      },
      "source": [
        "x_train"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['good',\n",
              " 'bad',\n",
              " 'happy',\n",
              " 'sad',\n",
              " 'not good',\n",
              " 'not bad',\n",
              " 'not happy',\n",
              " 'not sad',\n",
              " 'very good',\n",
              " 'very bad',\n",
              " 'very happy',\n",
              " 'very sad',\n",
              " 'i am happy',\n",
              " 'this is good',\n",
              " 'i am bad',\n",
              " 'this is bad',\n",
              " 'i am sad',\n",
              " 'this is sad',\n",
              " 'i am not happy',\n",
              " 'this is not good',\n",
              " 'i am not bad',\n",
              " 'this is not sad',\n",
              " 'i am very happy',\n",
              " 'this is very good',\n",
              " 'i am very bad',\n",
              " 'this is very sad',\n",
              " 'this is very happy',\n",
              " 'i am good not bad',\n",
              " 'this is good not bad',\n",
              " 'i am bad not good',\n",
              " 'i am good and happy',\n",
              " 'this is not good and not happy',\n",
              " 'i am not at all good',\n",
              " 'i am not at all bad',\n",
              " 'i am not at all happy',\n",
              " 'this is not at all sad',\n",
              " 'this is not at all happy',\n",
              " 'i am good right now',\n",
              " 'i am bad right now',\n",
              " 'this is bad right now',\n",
              " 'i am sad right now',\n",
              " 'i was good earlier',\n",
              " 'i was happy earlier',\n",
              " 'i was bad earlier',\n",
              " 'i was sad earlier',\n",
              " 'i am very bad right now',\n",
              " 'this is very good right now',\n",
              " 'this is very sad right now',\n",
              " 'this was bad earlier',\n",
              " 'this was very good earlier',\n",
              " 'this was very bad earlier',\n",
              " 'this was very happy earlier',\n",
              " 'this was very sad earlier',\n",
              " 'i was good and not bad earlier',\n",
              " 'i was not good and not happy earlier',\n",
              " 'i am not at all bad or sad right now',\n",
              " 'i am not at all good or happy right now',\n",
              " 'this was not happy and not good earlier']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owCZLsCebAni",
        "colab_type": "code",
        "outputId": "3a9af3ea-761b-489a-e7cf-705916967179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, hidden, K, vocab_size):\n",
        "        super(RNN, self).__init__()\n",
        "        \n",
        "        self.linearH = nn.Linear(vocab_size + hidden, hidden)\n",
        "        self.linearO = nn.Linear(vocab_size + hidden, K)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        x = torch.cat((x, h), 1)\n",
        "        h = torch.tanh(self.linearH(x))\n",
        "        out = torch.sigmoid(self.linearO(x))\n",
        "        \n",
        "        return out, h\n",
        "      \n",
        "      \n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, hidden, output_size, vocab_size):\n",
        "    super(LSTM, self).__init__()\n",
        "    \n",
        "    self.linearFt = nn.Linear(vocab_size + hidden, hidden)\n",
        "    self.linearIt = nn.Linear(vocab_size + hidden, hidden)\n",
        "    self.linearCt1 = nn.Linear(vocab_size + hidden, hidden)\n",
        "    self.linearOt = nn.Linear(vocab_size + hidden, hidden)\n",
        "    self.linearO = nn.Linear(vocab_size + hidden, output_size)\n",
        "    \n",
        "  def forward(self, x, h, c):\n",
        "    combined = torch.cat((x, h), 1)\n",
        "    ft = torch.sigmoid(self.linearFt(combined))\n",
        "    it = torch.sigmoid(self.linearIt(combined))\n",
        "    ct1 = torch.tanh(self.linearCt1(combined))\n",
        "    c = (ft*c) + (it * ct1)\n",
        "    \n",
        "    ot = torch.sigmoid(self.linearOt(combined))\n",
        "    h = ot * torch.tanh(c)\n",
        "    \n",
        "    out = torch.sigmoid(self.linearO(combined))\n",
        "    \n",
        "    return out, h, c\n",
        "\n",
        "class GRU(nn.Module):\n",
        "  \n",
        "  def __init__(self, hidden, output_size, vocab_size):\n",
        "    super(GRU, self).__init__()\n",
        "    \n",
        "    self.linearZt = nn.Linear(vocab_size + hidden, hidden)\n",
        "    self.linearRt = nn.Linear(vocab_size + hidden, hidden)\n",
        "    self.linearHt = nn.Linear(vocab_size + hidden, hidden)\n",
        "    \n",
        "    self.linearO = nn.Linear(vocab_size + hidden, output_size)\n",
        "    \n",
        "  def forward(self, x, h):\n",
        "    combined = torch.cat((x, h), 1)\n",
        "    zt = torch.sigmoid(self.linearZt(combined))\n",
        "    rt = torch.sigmoid(self.linearRt(combined))\n",
        "    combined2 = torch.cat((x, rt*h), 1)\n",
        "    ht = torch.tanh(self.linearHt(combined2))\n",
        "    h = (1-zt) * h + zt * ht\n",
        "    \n",
        "    out = torch.sigmoid(self.linearO(combined))\n",
        "  \n",
        "    return out, h\n",
        "  \n",
        "  \n",
        "hidden = 3\n",
        "K = 1\n",
        "vocab_size = len(vocab)\n",
        "learning_rate = 0.5\n",
        "epochs = 5000\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "\n",
        "#model = RNN(hidden, K, vocab_size)\n",
        "model = GRU(hidden, K, vocab_size)\n",
        "model.cuda()\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1000, 2000, 3000, 4000], gamma=0.1)\n",
        "\n",
        "\n",
        "for e in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    testing_loss = 0.0\n",
        "    \n",
        "    for x, y in zip(x_train, y_train):\n",
        "      \n",
        "        h = torch.zeros([1, hidden], dtype=torch.float32, device='cuda')\n",
        "        c = torch.zeros([1, hidden], dtype=torch.float32, device='cuda')\n",
        "        y_tensor = torch.tensor(y, dtype=torch.float32, device='cuda')\n",
        "        inputs = createInputs(x)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        for w in inputs:\n",
        "            x_tensor = torch.tensor(w, dtype=torch.float32, device='cuda')\n",
        "            x_tensor = x_tensor.view(1,-1)\n",
        "            out, h = model(x_tensor, h)\n",
        "            #index = np.where(w==1)[0][0]\n",
        "            #print(idx_to_word[index])\n",
        "\n",
        "        loss = loss_fn(out, y_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    run_loss = running_loss/len(x_train)\n",
        "    train_loss.append(run_loss)\n",
        "    scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    test_count = 5\n",
        "    for j in range(test_count):\n",
        "        index = np.random.randint(0, len(test_data)-1)\n",
        "        h = torch.zeros([1, hidden], dtype=torch.float32, device='cuda')\n",
        "        c = torch.zeros([1, hidden], dtype=torch.float32, device='cuda')\n",
        "        line = createInputs(x_test[index])\n",
        "        for w in line:\n",
        "            w = torch.tensor(w.reshape(1,-1), dtype=torch.float32, device='cuda')\n",
        "            out, h = model(w, h)\n",
        "\n",
        "        loss = loss_fn(out, torch.tensor(int(y_test[index]), dtype=torch.float32).view(1,1).to('cuda'))\n",
        "        testing_loss += loss.item()\n",
        "    \n",
        "    avg_loss = testing_loss / test_count\n",
        "    test_loss.append(avg_loss)\n",
        "    \n",
        "    if e % 100 == 0:\n",
        "        print(e, ' Train Loss: ', run_loss, ' Test Loss: ', avg_loss)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:512: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0  Train Loss:  0.8455213755369186  Test Loss:  0.8408667087554932\n",
            "100  Train Loss:  0.37762670649079094  Test Loss:  0.28701190433930607\n",
            "200  Train Loss:  0.373026242161547  Test Loss:  0.996937096118927\n",
            "300  Train Loss:  0.3709644578369174  Test Loss:  0.36741158890072256\n",
            "400  Train Loss:  0.3697484662753313  Test Loss:  0.6553669616580009\n",
            "500  Train Loss:  0.36892575475062117  Test Loss:  0.47544659962877633\n",
            "600  Train Loss:  0.3683218296997626  Test Loss:  0.25219378031324596\n",
            "700  Train Loss:  0.3678538532965768  Test Loss:  0.5058298716787248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgZDPYYAbF3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQrlosi2Cko6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}